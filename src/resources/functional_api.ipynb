{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
    "from sklearn.metrics import roc_curve, auc,  precision_score, recall_score, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import PIL\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_time = pd.read_csv('../data/notime_df.csv')\n",
    "no_time = no_time.loc[:, ~no_time.columns.str.contains('^Unnamed')]\n",
    "no_time = no_time.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fire(object):\n",
    "    def __init__(self, df):\n",
    "        self.y = df.pop('Target')\n",
    "        self.X = df\n",
    "\n",
    "    def split(self):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, stratify=self.y)\n",
    "\n",
    "    def fit(self, model):\n",
    "        self.model = model\n",
    "        kf = KFold(n_splits=5)\n",
    "        \n",
    "        accuracy = []\n",
    "        precision = []\n",
    "        recall = []\n",
    "        f1 = []\n",
    "\n",
    "        for train_index, test_index in kf.split(self.X_train):\n",
    "            X_train_split, X_test_split = self.X_train.iloc[train_index], self.X_train.iloc[test_index]\n",
    "            y_train_split, y_test_split = self.y_train.iloc[train_index], self.y_train.iloc[test_index]\n",
    "            self.model.fit(X_train_split, y_train_split, verbose=True)\n",
    "            self.pred = self.model.predict(X_test_split)\n",
    "\n",
    "            assess = lambda method, val=y_test_split, pred=self.pred: method(val, pred)\n",
    "\n",
    "            accuracy.append(assess(accuracy_score))\n",
    "            precision.append(assess(precision_score))\n",
    "            recall.append(assess(recall_score))\n",
    "            f1.append(assess(f1_score))\n",
    "        \n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': 10,\n",
    "          'colsample_bytree': 0.4,\n",
    "          'subsample': 0.8,\n",
    "          'min_child_weight': 6,\n",
    "          'n_estimators': 1000, \n",
    "          'learning_rate': 0.01,\n",
    "          'lambda': 0.8,\n",
    "          'alpha': 0.5}\n",
    "boost = XGBClassifier()\n",
    "boost = XGBClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_boost = Fire(no_time)\n",
    "data_boost.split()\n",
    "fitted_boost = data_boost.fit(boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(fitted_boost, open(\"pima.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "norcal = pd.read_csv('../data/norcal.csv')\n",
    "norcal = norcal.loc[:, ~norcal.columns.str.contains('^Unnamed')]\n",
    "socal = pd.read_csv('../data/socal.csv')\n",
    "socal = socal.loc[:, ~socal.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input a date and select a region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinedModels():\n",
    "    region = input('Enter \"norcal\" or \"socal\":  ')\n",
    "    date = input('Enter a date between 1/1/2018 and 9/13/2020:  ')\n",
    "    boost = pickle.load(open(\"pima.pickle.dat\", \"rb\"))\n",
    "    xception = load_model('xception_50epoch.h5')\n",
    "    batch_size = 16\n",
    "    img_height = 256\n",
    "    img_width = 256\n",
    "    epochs=50\n",
    "    \n",
    "    if region == 'norcal':\n",
    "        norcal = pd.read_csv('../data/norcal.csv')\n",
    "        norcal = norcal.loc[:, ~norcal.columns.str.contains('^Unnamed')]\n",
    "        df = norcal[norcal['Date']==date]\n",
    "        df.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "        # reformat date: 9/1/2020 -> 2020-09-01\n",
    "        datetime = pd.to_datetime(date, format='%m/%d/%Y')\n",
    "        datetime = datetime.strftime('%Y-%m-%d')\n",
    "        # find it in cleaned_data image files\n",
    "        path = glob.glob(f'../data/cleaned_data/*/{datetime}_1.jpg')\n",
    "        path = ''.join(element for element in path)\n",
    "        image = tf.keras.preprocessing.image.load_img(path, interpolation=\"bilinear\")\n",
    "        input_arr = keras.preprocessing.image.img_to_array(image)\n",
    "        input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "        image_risk = xception.predict(input_arr,batch_size=batch_size)\n",
    "        \n",
    "    elif region == 'socal':\n",
    "        socal = pd.read_csv('../data/socal.csv')\n",
    "        socal = socal.loc[:, ~socal.columns.str.contains('^Unnamed')]\n",
    "        df = socal[socal['Date']==date]\n",
    "        df.drop('Date', axis=1, inplace=True)\n",
    "        \n",
    "        # reformat date: 9/1/2020 -> 2020-09-01\n",
    "        datetime = pd.to_datetime(date, format='%m/%d/%Y')\n",
    "        datetime = datetime.strftime('%Y-%m-%d')\n",
    "        # find it in cleaned_data image files\n",
    "        path = glob.glob(f'../data/cleaned_data/*/{datetime}.jpg')\n",
    "        path = ''.join(element for element in path)\n",
    "        image = tf.keras.preprocessing.image.load_img(path, interpolation=\"bilinear\")\n",
    "        input_arr = keras.preprocessing.image.img_to_array(image)\n",
    "        input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "        image_risk = xception.predict(input_arr,batch_size=batch_size)\n",
    "        \n",
    "    conditions_y = df.pop('Target')\n",
    "    conditions_y_pred = boost.predict(df)\n",
    "    conditions_risk = np.mean(conditions_y_pred)\n",
    "    \n",
    "    risk = .8*(image_risk.ravel()[0]) + 0.2*conditions_risk\n",
    "    print(image_risk, conditions_risk)\n",
    "    return print(f'Risk for fire in {region} on {date} was: {round(risk,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter \"norcal\" or \"socal\":  norcal\n"
     ]
    }
   ],
   "source": [
    "combinedModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '9/3/2020'\n",
    "datetime = pd.to_datetime(date, format='%m/%d/%Y')\n",
    "datetime = datetime.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-09-03'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = glob.glob(f'../data/cleaned_data/*/{datetime}_1.jpg')\n",
    "path = ''.join(element for element in path)\n",
    "image = tf.keras.preprocessing.image.load_img(path, interpolation=\"bilinear\")\n",
    "input_arr = keras.preprocessing.image.img_to_array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[185., 187., 186.],\n",
       "        [185., 187., 186.],\n",
       "        [186., 188., 187.],\n",
       "        ...,\n",
       "        [ 70.,  62.,  43.],\n",
       "        [ 70.,  62.,  43.],\n",
       "        [ 70.,  62.,  43.]],\n",
       "\n",
       "       [[187., 189., 188.],\n",
       "        [188., 190., 189.],\n",
       "        [188., 190., 189.],\n",
       "        ...,\n",
       "        [ 66.,  58.,  39.],\n",
       "        [ 66.,  58.,  39.],\n",
       "        [ 66.,  58.,  39.]],\n",
       "\n",
       "       [[190., 192., 191.],\n",
       "        [190., 192., 191.],\n",
       "        [189., 191., 190.],\n",
       "        ...,\n",
       "        [ 67.,  59.,  40.],\n",
       "        [ 67.,  59.,  40.],\n",
       "        [ 67.,  59.,  40.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[119., 122., 131.],\n",
       "        [117., 120., 129.],\n",
       "        [117., 120., 129.],\n",
       "        ...,\n",
       "        [119., 111.,  98.],\n",
       "        [119., 111.,  98.],\n",
       "        [120., 112.,  99.]],\n",
       "\n",
       "       [[112., 115., 124.],\n",
       "        [111., 114., 123.],\n",
       "        [111., 114., 123.],\n",
       "        ...,\n",
       "        [120., 112.,  99.],\n",
       "        [121., 113., 100.],\n",
       "        [122., 114., 101.]],\n",
       "\n",
       "       [[114., 117., 126.],\n",
       "        [112., 115., 124.],\n",
       "        [110., 113., 122.],\n",
       "        ...,\n",
       "        [121., 113., 100.],\n",
       "        [122., 114., 101.],\n",
       "        [123., 115., 102.]]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs: image and conditions\n",
    "# include date prior to both these inputs to specify\n",
    "# norcal or socal and where in conditions df to look\n",
    "\n",
    "date_input = keras.Input(input('Enter a date between 1/1/2018 and 9/13/2020:  '))\n",
    "region_input = keras.Input(input('Enter \"norcal\" or \"socal\":  '))\n",
    "\n",
    "image_input = keras.Input(\n",
    "    shape=(img_height, img_width, 3), \n",
    "    name=\"satellite_image\"\n",
    "    ) \n",
    "\n",
    "conditions_input = keras.Input(\n",
    "    shape=(None,), \n",
    "    name=\"conditions\")\n",
    "\n",
    "\n",
    "xception_model=load_model('xception_50epoch.h5')(image_input)\n",
    "xception = layers.xception_model()(image_input)\n",
    "\n",
    "probability1 = xception\n",
    "probability2 = \n",
    "\n",
    "fire_risk = layers.average(x)\n",
    "\n",
    "bigmodel = keras.Model(\n",
    "    inputs = [image_input, conditions_input, date_input]\n",
    "    outputs = fire_risk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
